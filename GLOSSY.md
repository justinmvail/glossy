# GLOSSY - Photo Cards with AI Handwriting

## What it is
Consumer app for sending photo cards with the user's own handwriting on the back, generated by AI and pen-plotted on real cards.

## Tech Stack

### App
- **Platform:** Flutter (iOS first)
- **AI Model:** SDT (CVPR 2023) - outputs stroke coordinates, runs quantized on-device
- **Handwriting Input:** ~15 characters on-screen ‚Üí rendered to 64x64 PNGs

### Backend
- **Cloud:** AWS free tier
  - HTTP API Gateway
  - Lambda
  - DynamoDB
  - S3
  - SES (Simple Email Service)
- **Domain:** glossy.ink (Cloudflare DNS)
- **Hosting:** S3 + CloudFront
- **Address Validation:** Geocodio (75k/month free, score > 0.8)

### Fulfillment
- **Plotter:** AxiDraw (pen plotter)
- **Process:** Manual from garage

## How it Works

1. User draws ~15 characters on-screen ‚Üí rendered to 64x64 PNGs
2. SDT encodes style, generates stroke coordinates for any message
3. Order sent to backend (strokes JSON + photo + address)
4. Justin pulls orders, plots on AxiDraw, mails cards

## Key Decisions

- ‚úÖ Custom handwriting is a must-have (not preset fonts)
- ‚úÖ Customer's address as return address (feels personal)
- ‚úÖ "Report a Problem" button ‚Üí free replacement
- ‚úÖ Referral program:
  - 1 referral = free card
  - 5 referrals = POD shirt
  - 10 referrals = hoodie

## Kill Criteria

**Pivot to B2B if <50 paying customers in 3 months**

## Risks

- Distribution challenges
- Seasonal demand
- Price sensitivity ($5 vs free text)
- Single plotter bottleneck

## Strengths

- ~$700 startup cost
- Technical moat (SDT on-device)
- Pivot optionality to B2B

## First Priority

**Test SDT handwriting output on real people before building the full app**

## Progress Log

### ‚úÖ Completed (Jan 21, 2026)

**SDT Model Setup:**
- Cloned SDT repository (CVPR 2023 - Style-Disentangled Transformer)
- Set up Python 3.11 environment with PyTorch 2.2.2
- Downloaded pre-trained models:
  - SDT checkpoint (261 MB)
  - Content encoder (69 MB)
- Patched code for macOS compatibility (removed CUDA dependencies)
- Model runs on Apple Silicon GPU (MPS)

**Verification:**
- Model loads successfully (67.8M parameters, ~259 MB)
- Generates stroke coordinates in correct format: `[dx, dy, pen_up, pen_down, pen_end]`
- Created test scripts for validation and visualization
- Output format confirmed compatible with AxiDraw plotter

### ‚ö†Ô∏è Issues Discovered

**Model Output Quality:**
- Current English model checkpoint produces illegible output
- Model appears trained primarily for Chinese/Japanese handwriting
- Synthetic font-based samples don't match model's expected input format
- Requires actual handwritten samples (pen/stylus input) vs. computer fonts

**Model Size:**
- 259 MB is too large for on-device mobile deployment
- Quantization required for iOS (target: <50 MB)
- May need to evaluate alternative approaches

### ‚ùå SDT Evaluation Results (Jan 21, 2026)

**Test 1: Synthetic Font Samples**
- Used computer-rendered Bradley Hand font (15 samples)
- Result: Complete garbage - random diagonal lines, illegible
- Conclusion: Model requires real pen/stylus handwriting data

**Test 2: Real CASIA English Dataset**
- Downloaded official English dataset (204 test writers, 62 characters)
- Generated 1,984 samples with real handwriting style references
- Result: **STILL ILLEGIBLE** - most characters unrecognizable
- Only ~1-2 out of 12 samples were borderline readable ('Z', maybe '7')
- Examples: 'w', '4', 'W', 'a', 'G', 'O' were messy, unrecognizable strokes

**Critical Blocker:**
- SDT does not produce legible English handwriting suitable for customer cards
- This is a **product-killing issue** - can't ship illegible handwriting
- The checkpoint may be primarily Chinese/Japanese trained
- English support appears to be poor despite claims in paper

**Decision:**
- ‚ùå SDT is NOT viable for GLOSSY (English handwriting)
- ‚úÖ Moving to One-DM evaluation (ECCV 2024, claims better results with 1 sample vs 15)

### ‚úÖ One-DM Evaluation Results (Jan 21, 2026)

**Model Setup:**
- Cloned One-DM repository (ECCV 2024 - One-Shot Diffusion Mimicker)
- Set up Python 3.11 environment with PyTorch 2.2.2
- Downloaded 3 model checkpoints (1.6 GB total):
  - One-DM-ckpt.pt (1.2 GB)
  - vae_HTR138.pth (337 MB)
  - RN18_class_10400.pth (63 MB)
- Downloaded IAM English dataset with text corpus files
- Created macOS-compatible test script (removed CUDA multi-GPU dependencies)
- Model runs on Apple Silicon GPU (MPS)

**Test: IAM English Dataset**
- Generated 5 sample words: 'accents', 'fifty', 'gross', 'Tea', 'whom'
- Used DDIM sampling with 50 timesteps
- Generation time: ~2-3 seconds per timestep, ~12 minutes total for 5 samples
- Output: Raster images (64x height pixels, grayscale PNG)

**Results: FAILED ‚ùå**
- **CRITICAL ISSUE: Character accuracy problems**
- Word "Tea" consistently rendered with wrong letters:
  - Expected: "Tea"
  - Generated: "alea", "atea", "ales", etc. (checked 10+ samples from different writers)
- Output is MORE legible than SDT (recognizable glyphs)
- BUT characters are WRONG - this is a product-killing issue

**Investigation Findings:**
- Created debug scripts to verify content encoder input
- Confirmed input glyphs are CORRECT: 'T' (idx 33), 'e' (idx 8), 'a' (idx 19)
- Verified with debug_content_v2.py - input visualization shows proper "Tea" glyphs
- Problem is in MODEL OUTPUT, not preprocessing
- Model systematically hallucinates/substitutes wrong characters
- This is a fundamental model accuracy issue

**Key Findings:**
- One-DM produces more legible output than SDT (actual letter shapes vs random strokes)
- Only requires 1 style sample (vs SDT's 15)
- Generates raster images instead of stroke coordinates
- Would need vectorization post-processing for AxiDraw plotter
- Model size: ~1.6 GB (server-side generation only)
- **BUT: Character accuracy makes it unusable for customer photo cards**

**Decision:**
- ‚ùå **One-DM is NOT VIABLE for GLOSSY**
- Character errors are unacceptable for personalized messages
- Cannot ship cards that say wrong words to customers

### üîÑ One-DM Training Attempt (Jan 22, 2026)

**Goal:** Re-train One-DM from scratch to see if custom training improves character accuracy.

**Hardware:** NVIDIA GTX 1660 Super (6GB VRAM)

#### Initial Attempt (batch_size=2, no AMP)

**Changes for 1660 Super (low VRAM):**
- Created `configs/IAM64_small.yml` with memory-optimized settings:
  - `SOLVER.TYPE: SGD` (instead of AdamW - lower memory footprint)
  - `TRAIN.IMS_PER_BATCH: 2` (small batch size)
  - `SOLVER.GRAD_L2_CLIP: 1.0` (gradient clipping)
  - `SOLVER.WARMUP_ITERS: 5000`
  - `DATA_LOADER.NUM_THREADS: 2`
- Changed `SNAPSHOT_BEGIN: 1` and `SNAPSHOT_ITERS: 1` (save checkpoint after each epoch)
- Training uses single GPU via `torchrun --nproc_per_node=1`

**Problem Discovered:** Small batch size (2) weakens NCE contrastive learning.
- One-DM authors use 4 GPUs √ó batch_size=2 = effective batch_size=8
- NCE loss requires negative pairs within each batch to learn writer style differentiation
- With batch_size=2, often only 0-1 negative pairs available ‚Üí weak style encoder training
- Reconstruction loss (main quality metric) unaffected, but style fidelity may suffer

#### Optimization Attempts (Jan 22, 2026)

**Goal:** Increase batch size to improve NCE contrastive learning.

**Approaches Tested:**

| Approach | Result | Issue |
|----------|--------|-------|
| AMP (Mixed Precision) | ‚ùå Failed | NaN errors in UNet attention layers |
| batch_size=4 + AdamW | ‚ùå OOM | Needs ~6.5GB |
| batch_size=2 + AdamW | ‚ùå OOM | Needs ~5.8GB |
| batch_size=2 + SGD | ‚úÖ Works | Uses ~5.0GB |

**AMP Investigation:**
- Created `trainer/trainer_amp.py` with GradScaler and autocast
- Tested various configurations (FP32 VAE encode, FP32 loss computation)
- Model produces NaN in UNet attention layers regardless of configuration
- Conclusion: One-DM's attention architecture is not AMP-compatible

**Memory Constraints:**
- GTX 1660 Super: 6GB VRAM
- One-DM with batch_size=2 + SGD: ~5.0GB (87% utilization)
- AdamW requires ~2x optimizer memory vs SGD (momentum + variance)
- No headroom for larger batches without AMP

**Final Working Config (`configs/IAM64_small.yml`):**
```yaml
SOLVER:
  BASE_LR: 0.0001
  EPOCHS: 100
  WARMUP_ITERS: 5000
  TYPE: SGD
  GRAD_L2_CLIP: 1.0
TRAIN:
  IMS_PER_BATCH: 2
```

**Training Command:**
```bash
cd /home/server/One-DM
CUDA_VISIBLE_DEVICES=0 python3 -m torch.distributed.run --nproc_per_node=1 \
    -- train.py --cfg configs/IAM64_small.yml --log sgd_100ep
```

**Impact:**
- Reconstruction loss: Unaffected (trains normally)
- NCE contrastive loss: Weak signal (only 0-1 negative pairs per batch)
- Expected result: Legible handwriting, potentially weaker style matching

**Location:** `/home/server/One-DM/`

**Status:** ~~Training in progress~~ Single epoch completed - see results below.

---

### üìä Single Epoch Training Results (Jan 23, 2026)

**Training Completed:**
- Epochs: 1/1 (23,310 iterations)
- Duration: ~2 hours
- Final MSE loss: ~0.02
- Checkpoint saved: `Saved/IAM64_small/epoch1_v2-20260122_215439/model/0-ckpt.pt` (1.2 GB)

**Test Generation:**
- Generated test words: "hello", "the", "quick", "fox"
- Sampling: DDIM with 50 timesteps
- Output: 64px height grayscale images

**Results: Model Undertrained ‚ö†Ô∏è**

| Aspect | Status |
|--------|--------|
| Image generation | ‚úÖ Works - produces grayscale output |
| Correct dimensions | ‚úÖ Works - width scales with word length |
| Stroke-like patterns | ‚úÖ Partial - some dark shapes visible |
| Legible handwriting | ‚ùå Failed - abstract blobs, no letters |

**Sample Outputs (1 epoch):**
- All words produce abstract gray/white blob patterns
- No recognizable letter shapes
- Model has learned basic image structure but not character formation

**Conclusion:**
- 1 epoch is insufficient for diffusion model convergence
- Need 50-100+ epochs for legible handwriting output
- GTX 1660 Super (6GB) is too slow (~2 hours/epoch = 100-200 hours total)
- **Cloud GPU training required for practical iteration**

---

### ‚òÅÔ∏è Vast.ai Cloud Training Plan (Jan 23, 2026)

**Why Cloud:**
- Local GPU (GTX 1660 Super, 6GB) limitations:
  - Only batch_size=2 fits in memory
  - ~2 hours per epoch
  - 100 epochs = 200+ hours (8+ days continuous)
  - OOM errors during testing/inference
- Cloud GPU benefits:
  - Larger batch sizes (better NCE contrastive learning)
  - 10-20x faster training
  - More VRAM for experimentation

**Selected Configuration:**

| Component | Choice | Reason |
|-----------|--------|--------|
| Provider | Vast.ai | Cheapest GPU rentals, pay-per-hour |
| Template | PyTorch (Vast) | Clean setup with CUDA + PyTorch pre-installed |
| GPU | RTX 4090 (24GB) | 4x VRAM, ~5x faster than 1660, good price/performance |
| Storage | 50GB+ | Dataset (~2GB) + checkpoints (~1.2GB each) + overhead |

**Estimated Specs with RTX 4090:**

| Setting | GTX 1660 (6GB) | RTX 4090 (24GB) |
|---------|----------------|-----------------|
| Batch size | 2 | 8-16 |
| Time/epoch | ~2 hours | ~20-30 min |
| 100 epochs | ~200 hours | ~35-50 hours |
| NCE pairs/batch | 0-1 | 28-120 |

**Cost Estimate:**
- RTX 4090 on Vast.ai: ~$0.30-0.50/hour
- 50 hours training: ~$15-25
- With experimentation: ~$30-50 total

**Setup Steps:**

1. **Create Vast.ai account** at https://vast.ai
2. **Add credits** ($25-50 recommended for experimentation)
3. **Select instance:**
   - Template: "PyTorch (Vast)"
   - GPU: RTX 4090 (24GB)
   - Disk: 50GB+
   - Region: Any (cheapest)
4. **Connect via SSH** (Vast.ai provides connection string)
5. **Upload data and code:**
   ```bash
   # From local machine
   scp -P <port> -r /home/server/One-DM root@<vast-ip>:/workspace/
   scp -P <port> /home/server/One-DM/Saved/IAM64_small/epoch1_v2-*/model/0-ckpt.pt root@<vast-ip>:/workspace/One-DM/pretrained/
   ```
6. **Install dependencies:**
   ```bash
   cd /workspace/One-DM
   pip install -r requirements.txt
   pip install diffusers transformers accelerate
   ```
7. **Create optimized config** (`configs/IAM64_4090.yml`):
   ```yaml
   SOLVER:
     BASE_LR: 0.0001
     EPOCHS: 100
     WARMUP_ITERS: 5000
     TYPE: AdamW  # Can use AdamW with more VRAM
     GRAD_L2_CLIP: 1.0
   TRAIN:
     IMS_PER_BATCH: 8  # 4x larger batch
     SNAPSHOT_ITERS: 10  # Save every 10 epochs
   ```
8. **Start training:**
   ```bash
   CUDA_VISIBLE_DEVICES=0 python3 -m torch.distributed.run --nproc_per_node=1 \
       -- train.py --cfg configs/IAM64_4090.yml --log cloud_run1
   ```
9. **Monitor progress:**
   ```bash
   # Watch training loss
   tail -f /workspace/One-DM/Saved/*/tboard/loss/*/events*

   # Or use TensorBoard
   tensorboard --logdir /workspace/One-DM/Saved/ --port 6006
   ```
10. **Download results** when done:
    ```bash
    # From local machine
    scp -P <port> root@<vast-ip>:/workspace/One-DM/Saved/*/model/*.pt ./checkpoints/
    ```

**Training Milestones to Watch:**
- Epoch 10: Should see emerging letter shapes
- Epoch 25: Letters should be mostly recognizable
- Epoch 50: Good quality, may be sufficient
- Epoch 100: Full convergence expected

**Next Steps:**
1. Create Vast.ai account and add credits
2. Rent RTX 4090 instance with PyTorch template
3. Upload dataset and 1-epoch checkpoint
4. Train for 50-100 epochs
5. Test character accuracy on validation set
6. If successful, evaluate for production viability

---

### ‚úÖ Vast.ai Training Results (Jan 23, 2026)

**Setup Completed:**
- Rented RTX 4090 (24GB) instance on Vast.ai
- Uploaded optimized training files via Google Drive
- Files committed to repo in `vast_setup/` directory

**Optimization Testing:**

| Configuration | Speed | GPU Util | Memory | Result |
|---------------|-------|----------|--------|--------|
| batch=8 (initial) | - | 7-13% | - | Too slow |
| batch=32 | - | ~23% | - | Still slow |
| batch=64 | ~15 min/epoch | - | - | Better |
| batch=128 | ~7 min/epoch | 63% | 18GB | Good |
| batch=128 + AMP | 1.94 s/it | 86% | 15.6GB | **Slower** (AMP overhead) |
| batch=256 + AMP | - | - | OOM | Out of memory |
| **batch=128 + TF32** | **1.20 s/it** | **100%** | **18.4GB** | **Optimal** |

**Key Findings:**
- AMP (mixed precision) was **slower** for this model (1.94 vs 1.20 s/it)
- TF32 alone provides benefit without AMP overhead
- torch.compile() caused hangs during graph compilation
- Gradient checkpointing can be disabled with 24GB VRAM

**Final Optimized Configuration:**
```yaml
# IAM64_4090.yml
SOLVER:
  BASE_LR: 0.0001
  EPOCHS: 100
  TYPE: AdamW
  GRAD_L2_CLIP: 1.0
TRAIN:
  IMS_PER_BATCH: 128
  SNAPSHOT_ITERS: 10
DATA_LOADER:
  NUM_THREADS: 16
```

**Performance:**
- Speed: ~1.20 s/it (365 iterations/epoch)
- Time per epoch: ~7.3 minutes
- GPU utilization: 100%
- Power draw: 262W / 400W (65%)
- Memory: 18.4GB / 24GB (75%)

**Cost Estimate (Revised):**
- 100 epochs √ó 7.3 min = ~12 hours
- RTX 4090 @ $0.40/hour = **~$5 total**
- Much cheaper than original $15-25 estimate

**Files in `vast_setup/`:**
- `train_4090.py` - TF32 + DataLoader optimizations
- `trainer_4090.py` - Simplified trainer (no AMP)
- `IAM64_4090.yml` - Optimized config
- `setup_vast.sh` - Manual setup script
- `onstart.sh` - Fully automated on-start script
- `QUICKSTART.md` - Documentation
- Note: `onedm_data.tar.gz` (246MB) hosted on Google Drive

**Google Drive Link:**
https://drive.google.com/drive/folders/1UY61ytrE6ec-OBdMESZvcpD9gcVsz_ad

**Status:** Training in progress on Vast.ai. Monitoring for 100 epoch completion.

---

### üîÑ Next Steps

**Completed Evaluations:**
1. ~~Test SDT with real English data~~ ‚úÖ Done - ‚ùå FAILED (illegible output)
2. ~~Evaluate One-DM (newer model, ECCV 2024)~~ ‚úÖ Done - ‚ùå FAILED (wrong characters)
3. ~~Research open-source handwriting AI models~~ ‚úÖ Done - 17 models identified

**Current Situation:**
- **Both major models failed quality tests**
- SDT: Completely illegible (random strokes)
- One-DM: Legible but wrong characters ("Tea" ‚Üí "alea")
- No single open-source model has: vector output + few-shot + high accuracy

**Options to Consider:**

1. **Test other models from research:**
   - DiffusionPen (AAAI 2024) - diffusion model with writer-aware features
   - FW-GAN (2022) - GAN-based with writer-conditioned generation
   - DiffBrush (2023) - diffusion with stroke-level control
   - Note: Most modern models output raster images, require vectorization

2. **Hybrid approach:**
   - Use raster-based model (if one has better accuracy than One-DM)
   - Add InkSight vectorization to convert images ‚Üí SVG strokes
   - Concern: Doesn't solve One-DM's character accuracy problem

3. **Traditional approach (no few-shot):**
   - handwriting-synthesis (Graves 2013) - direct vector output
   - Requires retraining per user (~2-3 hours GPU per person)
   - Removes "personalized style" selling point or adds cost/latency

4. **Non-AI alternatives:**
   - Font-based with handwriting-style fonts
   - Manual tracing of user handwriting samples
   - Partner with existing handwriting API services

**Priority: Determine viability before building infrastructure**

### üìä Handwriting AI Models Research (Jan 21, 2026)

**Criteria:**
- Open-source/free
- Few-shot capable (1-20 samples without retraining)
- Vector/stroke output OR high-quality raster
- English handwriting support

**17 Models Identified:**

| Model | Year | Output | Few-shot | Status | Notes |
|-------|------|--------|----------|--------|-------|
| **SDT** | 2023 | Strokes | ‚ùå (needs 15) | ‚ùå FAILED | Illegible English output |
| **One-DM** | 2024 | Raster | ‚úÖ (1 sample) | ‚ùå FAILED | Wrong characters ("Tea"‚Üí"alea") |
| **DiffusionPen** | 2024 | Raster | ‚úÖ (few-shot) | Not tested | SOTA quality claimed |
| **InkSight** | 2024 | Strokes | ‚ùå | Not tested | Vectorization tool, not generator |
| **DiffBrush** | 2023 | Raster | ‚úÖ | Not tested | Stroke-level control |
| **Diff-Handwriting** | 2023 | Raster | ‚úÖ | Not tested | Diffusion-based |
| **FW-GAN** | 2022 | Raster | ‚úÖ | Not tested | GAN-based |
| **GANwriting** | 2020 | Raster | ‚úÖ | Not tested | GAN-based |
| **HWT** | 2022 | Raster | ‚ùå (paired) | Not tested | Paired style transfer |
| **SmartPatch** | 2023 | Raster | ‚ùå | Not tested | Patch-based |
| **handwriting-synthesis** | 2013 | Strokes | ‚ùå (retrain) | Not tested | Graves RNN, direct vector |
| **DeepWriting** | 2016 | Strokes | ‚ùå (retrain) | Not tested | Extended Graves model |
| **Handwriting Transformer** | 2021 | Strokes | ‚ùå (retrain) | Not tested | Transformer-based |
| **Text-to-Handwriting** | 2020 | Raster | ‚ùå | Not tested | Pix2pix GAN |
| **VATr** | 2021 | Raster | ‚ùå | Not tested | Vision-audio transformer |
| **Write Like Me** | 2021 | Raster | ‚ùå | Not tested | Commercial API reference |
| **CalliGAN** | 2020 | Raster | ‚ùå | Not tested | Calligraphy-focused |

**Key Insights:**
- **Modern models** (2023-2024): Diffusion-based, raster output, best quality, few-shot capable
- **Classic models** (2013-2016): RNN-based, stroke output, require full retraining
- **No perfect match**: No open-source model has vector + few-shot + proven accuracy
- **Hybrid approach needed**: Best modern model + vectorization tool (InkSight)

**Concern:** One-DM (the most recent diffusion model) already failed accuracy tests. Other diffusion models may have similar issues.

## Project Structure

```
glossy/
‚îú‚îÄ‚îÄ app/                    # Flutter mobile app (not started)
‚îú‚îÄ‚îÄ backend/                # AWS Lambda functions (not started)
‚îú‚îÄ‚îÄ docs/                   # Additional documentation
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ SDT/                # SDT model (CVPR 2023) - ‚ùå English output illegible
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_zoo/      # Pre-trained models (261 MB + 69 MB)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data/           # CASIA English dataset (204 writers)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Generated/      # Test output (1,984 samples - illegible)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ venv/           # Python 3.11 environment
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *.py            # Test and visualization scripts
‚îÇ   ‚îî‚îÄ‚îÄ One-DM/             # One-DM model (ECCV 2024) - ‚úÖ SUCCESS (legible output!)
‚îÇ       ‚îú‚îÄ‚îÄ model_zoo/      # Pre-trained models (1.6 GB - downloaded)
‚îÇ       ‚îú‚îÄ‚îÄ data/           # IAM English dataset + text corpus
‚îÇ       ‚îú‚îÄ‚îÄ Generated/      # Test output (5 samples - all legible!)
‚îÇ       ‚îú‚îÄ‚îÄ venv/           # Python 3.11 environment
‚îÇ       ‚îú‚îÄ‚îÄ test_macos.py   # macOS-compatible test script
‚îÇ       ‚îî‚îÄ‚îÄ *.py            # Model scripts
‚îî‚îÄ‚îÄ scripts/                # Utility scripts (not started)
```

## Technical Notes

**SDT Model Details:**
- Input: 15 style samples (64x64 grayscale) + target character (64x64)
- Output: Up to 120 stroke points with 5D coordinates
- Coordinate format: Incremental (dx, dy) + pen state (up/down/end)
- For AxiDraw: Convert to absolute coordinates via cumulative sum

**Environment:**
- Python: 3.11.14
- PyTorch: 2.2.2 (with MPS support for Apple Silicon)
- Device: macOS with Apple Silicon GPU
- Dependencies: numpy 1.26.4, opencv-python 4.5.5, matplotlib 3.7.5

**Repository:**
- GitHub: https://github.com/justinmvail/glossy
- Created: Jan 21, 2026
- Current status: Model evaluation phase
