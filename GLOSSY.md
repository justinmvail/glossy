# GLOSSY - Photo Cards with AI Handwriting

## What it is
Consumer app for sending photo cards with the user's own handwriting on the back, generated by AI and pen-plotted on real cards.

## Tech Stack

### App
- **Platform:** Flutter (iOS first)
- **AI Model:** One-DM (ECCV 2024) - diffusion-based, raster output, requires server or on-device quantization
- **Handwriting Input:** ~15 characters on-screen â†’ rendered to 64x64 PNGs
- **Note:** SDT (CVPR 2023) was evaluated but has no English checkpoint (Chinese/Japanese only)

### Backend
- **Cloud:** AWS free tier
  - HTTP API Gateway
  - Lambda
  - DynamoDB
  - S3
  - SES (Simple Email Service)
- **Domain:** glossy.ink (Cloudflare DNS)
- **Hosting:** S3 + CloudFront
- **Address Validation:** Geocodio (75k/month free, score > 0.8)

### Fulfillment
- **Plotter:** AxiDraw (pen plotter)
- **Process:** Manual from garage

## How it Works

1. User draws ~15 characters on-screen â†’ rendered to 64x64 PNGs
2. One-DM encodes style, generates handwriting image for any message
3. Vectorize raster output for AxiDraw (or use "Magic Reveal" streaming UX)
4. Order sent to backend (image/strokes + photo + address)
5. Justin pulls orders, plots on AxiDraw, mails cards

## Key Decisions

- âœ… Custom handwriting is a must-have (not preset fonts)
- âœ… Customer's address as return address (feels personal)
- âœ… "Report a Problem" button â†’ free replacement
- âœ… Referral program:
  - 1 referral = free card
  - 5 referrals = POD shirt
  - 10 referrals = hoodie

## Kill Criteria

**Pivot to B2B if <50 paying customers in 3 months**

## Risks

- Distribution challenges
- Seasonal demand
- Price sensitivity ($5 vs free text)
- Single plotter bottleneck

## Strengths

- ~$700 startup cost
- Technical moat (custom-trained One-DM)
- Pivot optionality to B2B

## First Priority

**Complete One-DM training and test handwriting output on real people before building the full app**

## Progress Log

### âœ… Completed (Jan 21, 2026)

**SDT Model Setup:**
- Cloned SDT repository (CVPR 2023 - Style-Disentangled Transformer)
- Set up Python 3.11 environment with PyTorch 2.2.2
- Downloaded pre-trained models:
  - SDT checkpoint (261 MB)
  - Content encoder (69 MB)
- Patched code for macOS compatibility (removed CUDA dependencies)
- Model runs on Apple Silicon GPU (MPS)

**Verification:**
- Model loads successfully (67.8M parameters, ~259 MB)
- Generates stroke coordinates in correct format: `[dx, dy, pen_up, pen_down, pen_end]`
- Created test scripts for validation and visualization
- Output format confirmed compatible with AxiDraw plotter

### âš ï¸ Issues Discovered

**Model Output Quality:**
- **SDT has NO English checkpoint** (confirmed via [GitHub Issue #102](https://github.com/dailenson/SDT/issues/102))
- Pre-trained models are Chinese and Japanese ONLY
- Maintainer explicitly points English users to One-DM instead
- Testing with Chinese/Japanese checkpoint on English data produces garbage

**Model Size:**
- 259 MB is too large for on-device mobile deployment
- Quantization required for iOS (target: <50 MB)
- May need to evaluate alternative approaches

### âŒ SDT Evaluation Results (Jan 21, 2026)

**Test 1: Synthetic Font Samples**
- Used computer-rendered Bradley Hand font (15 samples)
- Result: Complete garbage - random diagonal lines, illegible
- Conclusion: Model requires real pen/stylus handwriting data

**Test 2: Real CASIA English Dataset**
- Downloaded official English dataset (204 test writers, 62 characters)
- Generated 1,984 samples with real handwriting style references
- Result: **STILL ILLEGIBLE** - most characters unrecognizable
- Only ~1-2 out of 12 samples were borderline readable ('Z', maybe '7')
- Examples: 'w', '4', 'W', 'a', 'G', 'O' were messy, unrecognizable strokes

**Critical Blocker:**
- SDT does not produce legible English handwriting suitable for customer cards
- This is a **product-killing issue** - can't ship illegible handwriting
- **Root cause: No English checkpoint exists** ([GitHub Issue #102](https://github.com/dailenson/SDT/issues/102))
- Pre-trained SDT only supports Chinese and Japanese
- Maintainer's response: "The English checkpoint is in this repository: https://github.com/dailenson/One-DM"

**Decision:**
- âŒ SDT is NOT viable for GLOSSY (no English model available)
- âœ… Moving to One-DM evaluation (ECCV 2024, has English support)

### âœ… One-DM Evaluation Results (Jan 21, 2026)

**Model Setup:**
- Cloned One-DM repository (ECCV 2024 - One-Shot Diffusion Mimicker)
- Set up Python 3.11 environment with PyTorch 2.2.2
- Downloaded 3 model checkpoints (1.6 GB total):
  - One-DM-ckpt.pt (1.2 GB)
  - vae_HTR138.pth (337 MB)
  - RN18_class_10400.pth (63 MB)
- Downloaded IAM English dataset with text corpus files
- Created macOS-compatible test script (removed CUDA multi-GPU dependencies)
- Model runs on Apple Silicon GPU (MPS)

**Test: IAM English Dataset**
- Generated 5 sample words: 'accents', 'fifty', 'gross', 'Tea', 'whom'
- Used DDIM sampling with 50 timesteps
- Generation time: ~2-3 seconds per timestep, ~12 minutes total for 5 samples
- Output: Raster images (64x height pixels, grayscale PNG)

**Results: FAILED âŒ**
- **CRITICAL ISSUE: Character accuracy problems**
- Word "Tea" consistently rendered with wrong letters:
  - Expected: "Tea"
  - Generated: "alea", "atea", "ales", etc. (checked 10+ samples from different writers)
- Output is MORE legible than SDT (recognizable glyphs)
- BUT characters are WRONG - this is a product-killing issue

**Investigation Findings:**
- Created debug scripts to verify content encoder input
- Confirmed input glyphs are CORRECT: 'T' (idx 33), 'e' (idx 8), 'a' (idx 19)
- Verified with debug_content_v2.py - input visualization shows proper "Tea" glyphs
- Problem is in MODEL OUTPUT, not preprocessing
- Model systematically hallucinates/substitutes wrong characters
- This is a fundamental model accuracy issue

**Key Findings:**
- One-DM produces more legible output than SDT (actual letter shapes vs random strokes)
- Only requires 1 style sample (vs SDT's 15)
- Generates raster images instead of stroke coordinates
- Would need vectorization post-processing for AxiDraw plotter
- Model size: ~1.6 GB (server-side generation only)
- **BUT: Character accuracy makes it unusable for customer photo cards**

**Decision:**
- âŒ **One-DM is NOT VIABLE for GLOSSY**
- Character errors are unacceptable for personalized messages
- Cannot ship cards that say wrong words to customers

### ðŸ”„ One-DM Training Attempt (Jan 22, 2026)

**Goal:** Re-train One-DM from scratch to see if custom training improves character accuracy.

**Hardware:** NVIDIA GTX 1660 Super (6GB VRAM)

#### Initial Attempt (batch_size=2, no AMP)

**Changes for 1660 Super (low VRAM):**
- Created `configs/IAM64_small.yml` with memory-optimized settings:
  - `SOLVER.TYPE: SGD` (instead of AdamW - lower memory footprint)
  - `TRAIN.IMS_PER_BATCH: 2` (small batch size)
  - `SOLVER.GRAD_L2_CLIP: 1.0` (gradient clipping)
  - `SOLVER.WARMUP_ITERS: 5000`
  - `DATA_LOADER.NUM_THREADS: 2`
- Changed `SNAPSHOT_BEGIN: 1` and `SNAPSHOT_ITERS: 1` (save checkpoint after each epoch)
- Training uses single GPU via `torchrun --nproc_per_node=1`

**Problem Discovered:** Small batch size (2) weakens NCE contrastive learning.
- One-DM authors use 4 GPUs Ã— batch_size=2 = effective batch_size=8
- NCE loss requires negative pairs within each batch to learn writer style differentiation
- With batch_size=2, often only 0-1 negative pairs available â†’ weak style encoder training
- Reconstruction loss (main quality metric) unaffected, but style fidelity may suffer

#### Optimization Attempts (Jan 22, 2026)

**Goal:** Increase batch size to improve NCE contrastive learning.

**Approaches Tested:**

| Approach | Result | Issue |
|----------|--------|-------|
| AMP (Mixed Precision) | âŒ Failed | NaN errors in UNet attention layers |
| batch_size=4 + AdamW | âŒ OOM | Needs ~6.5GB |
| batch_size=2 + AdamW | âŒ OOM | Needs ~5.8GB |
| batch_size=2 + SGD | âœ… Works | Uses ~5.0GB |

**AMP Investigation:**
- Created `trainer/trainer_amp.py` with GradScaler and autocast
- Tested various configurations (FP32 VAE encode, FP32 loss computation)
- Model produces NaN in UNet attention layers regardless of configuration
- Conclusion: One-DM's attention architecture is not AMP-compatible

**Memory Constraints:**
- GTX 1660 Super: 6GB VRAM
- One-DM with batch_size=2 + SGD: ~5.0GB (87% utilization)
- AdamW requires ~2x optimizer memory vs SGD (momentum + variance)
- No headroom for larger batches without AMP

**Final Working Config (`configs/IAM64_small.yml`):**
```yaml
SOLVER:
  BASE_LR: 0.0001
  EPOCHS: 100
  WARMUP_ITERS: 5000
  TYPE: SGD
  GRAD_L2_CLIP: 1.0
TRAIN:
  IMS_PER_BATCH: 2
```

**Training Command:**
```bash
cd /home/server/One-DM
CUDA_VISIBLE_DEVICES=0 python3 -m torch.distributed.run --nproc_per_node=1 \
    -- train.py --cfg configs/IAM64_small.yml --log sgd_100ep
```

**Impact:**
- Reconstruction loss: Unaffected (trains normally)
- NCE contrastive loss: Weak signal (only 0-1 negative pairs per batch)
- Expected result: Legible handwriting, potentially weaker style matching

**Location:** `/home/server/One-DM/`

**Status:** ~~Training in progress~~ Single epoch completed - see results below.

---

### ðŸ“Š Single Epoch Training Results (Jan 23, 2026)

**Training Completed:**
- Epochs: 1/1 (23,310 iterations)
- Duration: ~2 hours
- Final MSE loss: ~0.02
- Checkpoint saved: `Saved/IAM64_small/epoch1_v2-20260122_215439/model/0-ckpt.pt` (1.2 GB)

**Test Generation:**
- Generated test words: "hello", "the", "quick", "fox"
- Sampling: DDIM with 50 timesteps
- Output: 64px height grayscale images

**Results: Model Undertrained âš ï¸**

| Aspect | Status |
|--------|--------|
| Image generation | âœ… Works - produces grayscale output |
| Correct dimensions | âœ… Works - width scales with word length |
| Stroke-like patterns | âœ… Partial - some dark shapes visible |
| Legible handwriting | âŒ Failed - abstract blobs, no letters |

**Sample Outputs (1 epoch):**
- All words produce abstract gray/white blob patterns
- No recognizable letter shapes
- Model has learned basic image structure but not character formation

**Conclusion:**
- 1 epoch is insufficient for diffusion model convergence
- Need 50-100+ epochs for legible handwriting output
- GTX 1660 Super (6GB) is too slow (~2 hours/epoch = 100-200 hours total)
- **Cloud GPU training required for practical iteration**

---

### â˜ï¸ Vast.ai Cloud Training Plan (Jan 23, 2026)

**Why Cloud:**
- Local GPU (GTX 1660 Super, 6GB) limitations:
  - Only batch_size=2 fits in memory
  - ~2 hours per epoch
  - 100 epochs = 200+ hours (8+ days continuous)
  - OOM errors during testing/inference
- Cloud GPU benefits:
  - Larger batch sizes (better NCE contrastive learning)
  - 10-20x faster training
  - More VRAM for experimentation

**Selected Configuration:**

| Component | Choice | Reason |
|-----------|--------|--------|
| Provider | Vast.ai | Cheapest GPU rentals, pay-per-hour |
| Template | PyTorch (Vast) | Clean setup with CUDA + PyTorch pre-installed |
| GPU | RTX 4090 (24GB) | 4x VRAM, ~5x faster than 1660, good price/performance |
| Storage | 50GB+ | Dataset (~2GB) + checkpoints (~1.2GB each) + overhead |

**Estimated Specs with RTX 4090:**

| Setting | GTX 1660 (6GB) | RTX 4090 (24GB) |
|---------|----------------|-----------------|
| Batch size | 2 | 8-16 |
| Time/epoch | ~2 hours | ~20-30 min |
| 100 epochs | ~200 hours | ~35-50 hours |
| NCE pairs/batch | 0-1 | 28-120 |

**Cost Estimate:**
- RTX 4090 on Vast.ai: ~$0.30-0.50/hour
- 50 hours training: ~$15-25
- With experimentation: ~$30-50 total

**Setup Steps:**

1. **Create Vast.ai account** at https://vast.ai
2. **Add credits** ($25-50 recommended for experimentation)
3. **Select instance:**
   - Template: "PyTorch (Vast)"
   - GPU: RTX 4090 (24GB)
   - Disk: 50GB+
   - Region: Any (cheapest)
4. **Connect via SSH** (Vast.ai provides connection string)
5. **Upload data and code:**
   ```bash
   # From local machine
   scp -P <port> -r /home/server/One-DM root@<vast-ip>:/workspace/
   scp -P <port> /home/server/One-DM/Saved/IAM64_small/epoch1_v2-*/model/0-ckpt.pt root@<vast-ip>:/workspace/One-DM/pretrained/
   ```
6. **Install dependencies:**
   ```bash
   cd /workspace/One-DM
   pip install -r requirements.txt
   pip install diffusers transformers accelerate
   ```
7. **Create optimized config** (`configs/IAM64_4090.yml`):
   ```yaml
   SOLVER:
     BASE_LR: 0.0001
     EPOCHS: 100
     WARMUP_ITERS: 5000
     TYPE: AdamW  # Can use AdamW with more VRAM
     GRAD_L2_CLIP: 1.0
   TRAIN:
     IMS_PER_BATCH: 8  # 4x larger batch
     SNAPSHOT_ITERS: 10  # Save every 10 epochs
   ```
8. **Start training:**
   ```bash
   CUDA_VISIBLE_DEVICES=0 python3 -m torch.distributed.run --nproc_per_node=1 \
       -- train.py --cfg configs/IAM64_4090.yml --log cloud_run1
   ```
9. **Monitor progress:**
   ```bash
   # Watch training loss
   tail -f /workspace/One-DM/Saved/*/tboard/loss/*/events*

   # Or use TensorBoard
   tensorboard --logdir /workspace/One-DM/Saved/ --port 6006
   ```
10. **Download results** when done:
    ```bash
    # From local machine
    scp -P <port> root@<vast-ip>:/workspace/One-DM/Saved/*/model/*.pt ./checkpoints/
    ```

**Training Milestones to Watch:**
- Epoch 10: Should see emerging letter shapes
- Epoch 25: Letters should be mostly recognizable
- Epoch 50: Good quality, may be sufficient
- Epoch 100: Full convergence expected

**Next Steps:**
1. Create Vast.ai account and add credits
2. Rent RTX 4090 instance with PyTorch template
3. Upload dataset and 1-epoch checkpoint
4. Train for 50-100 epochs
5. Test character accuracy on validation set
6. If successful, evaluate for production viability

---

### âœ… Vast.ai Training Results (Jan 23, 2026)

**Setup Completed:**
- Rented RTX 4090 (24GB) instance on Vast.ai
- Uploaded optimized training files via Google Drive
- Files committed to repo in `vast_setup/` directory

**Optimization Testing:**

| Configuration | Speed | GPU Util | Memory | Result |
|---------------|-------|----------|--------|--------|
| batch=8 (initial) | - | 7-13% | - | Too slow |
| batch=32 | - | ~23% | - | Still slow |
| batch=64 | ~15 min/epoch | - | - | Better |
| batch=128 | ~7 min/epoch | 63% | 18GB | Good |
| batch=128 + AMP | 1.94 s/it | 86% | 15.6GB | **Slower** (AMP overhead) |
| batch=256 + AMP | - | - | OOM | Out of memory |
| **batch=128 + TF32** | **1.20 s/it** | **100%** | **18.4GB** | **Optimal** |

**Key Findings:**
- AMP (mixed precision) was **slower** for this model (1.94 vs 1.20 s/it)
- TF32 alone provides benefit without AMP overhead
- torch.compile() caused hangs during graph compilation
- Gradient checkpointing can be disabled with 24GB VRAM

**Final Optimized Configuration:**
```yaml
# IAM64_4090.yml
SOLVER:
  BASE_LR: 0.0001
  EPOCHS: 100
  TYPE: AdamW
  GRAD_L2_CLIP: 1.0
TRAIN:
  IMS_PER_BATCH: 128
  SNAPSHOT_ITERS: 10
DATA_LOADER:
  NUM_THREADS: 16
```

**Performance:**
- Speed: ~1.20 s/it (365 iterations/epoch)
- Time per epoch: ~7.3 minutes
- GPU utilization: 100%
- Power draw: 262W / 400W (65%)
- Memory: 18.4GB / 24GB (75%)

**Cost Estimate (Revised):**
- 100 epochs Ã— 7.3 min = ~12 hours
- RTX 4090 @ $0.40/hour = **~$5 total**
- Much cheaper than original $15-25 estimate

**Files in `vast_setup/`:**
- `train_4090.py` - TF32 + DataLoader optimizations
- `trainer_4090.py` - Simplified trainer (no AMP)
- `IAM64_4090.yml` - Optimized config
- `setup_vast.sh` - Manual setup script
- `onstart.sh` - Fully automated on-start script
- `QUICKSTART.md` - Documentation
- Note: `onedm_data.tar.gz` (246MB) hosted on Google Drive

**Google Drive Link:**
https://drive.google.com/drive/folders/1UY61ytrE6ec-OBdMESZvcpD9gcVsz_ad

**Status:** Training in progress on Vast.ai (~epoch 30/100 as of Jan 23, 11pm).

**Actual Cost (Jan 23, 2026):**
- Instance rate: $0.217/hr (cheaper than estimated $0.40)
- Setup/optimization: ~$0.43 (2 hours)
- Training: ~$2.60 (12 hours)
- **Total: ~$3.00** for 100 epochs

---

### ðŸ”„ Next Steps / TODO

**Completed:**
1. ~~Test SDT with real English data~~ âœ… Done - âŒ FAILED (illegible output)
2. ~~Evaluate One-DM (newer model, ECCV 2024)~~ âœ… Done - âŒ FAILED (wrong characters with pretrained)
3. ~~Research open-source handwriting AI models~~ âœ… Done - 17 models identified
4. ~~Set up Vast.ai cloud training~~ âœ… Done - Training in progress
5. ~~Optimize training (batch size, TF32)~~ âœ… Done - 100% GPU utilization

**In Progress:**
- [ ] Complete 100 epoch training on Vast.ai
- [ ] Test trained model for character accuracy

**TODO After Training:**
- [ ] Test epoch 10/20/30 checkpoints for quality
- [ ] If quality good: proceed with deployment planning
- [ ] Convert model to Core ML (iOS) / TFLite (Android)
- [ ] INT8 quantization (~1.2GB â†’ ~300MB)
- [ ] Build Flutter inference prototype

**âŒ SDT Word-Level Training (Evaluated Jan 24, 2026 - NOT VIABLE)**
- SDT has no English checkpoint ([GitHub Issue #102](https://github.com/dailenson/SDT/issues/102))
- Architecture is hardcoded for single 64x64 character input
- Content encoder uses mean pooling â†’ cannot represent multiple characters
- Would require significant architecture modifications, not just training data
- IAM-OnDB has word-level stroke data (86K words, 221 writers) but SDT can't use it as-is
- **Decision:** Stick with One-DM training (already handles variable-length words)

---

### ðŸ“± On-Device Inference Plan (Jan 24, 2026)

**Concept: "Magic Reveal" UX**

Instead of hiding inference behind a spinner, show the diffusion process live:
1. User sees word emerge from noise (step by step)
2. Each DDIM step updates the display in real-time
3. Vectorization animates pen drawing over final image
4. Word flies into position on card

**Why this is better:**
- 4-8 second inference feels instant when user watches it happen
- Unique UX differentiator vs competitors
- "Magic moment" creates emotional connection

**Technical Requirements:**

| Component | Original | Quantized (INT8) |
|-----------|----------|------------------|
| UNet | 1.2 GB | ~300 MB |
| VAE Decoder | 337 MB | ~85 MB |
| **Total** | **1.6 GB** | **~400 MB** |

**Expected Performance (on-device):**

| Device | Per Step | 20 Steps Total |
|--------|----------|----------------|
| iPhone 15 Pro | ~200ms | ~4 sec |
| iPhone 13 | ~400ms | ~8 sec |
| Pixel 8 Pro | ~300ms | ~6 sec |
| Mid-range (2022+) | ~800ms | ~16 sec |

**Implementation Steps:**
1. Export trained model to ONNX
2. Convert to Core ML (iOS) / TFLite (Android)
3. Quantize to INT8
4. Flutter plugin for native inference
5. Build step-by-step preview UI

**Alternative: Server-Side with Streaming**

If on-device is too slow/large, use server with WebSocket streaming:
- AWS SageMaker Async Inference (scales to zero)
- Replicate.com (easiest, ~$0.02/generation)
- Modal.com (Python-native serverless GPU)

Stream each diffusion step to frontend for same "magic reveal" effect.

---

### ðŸ’¡ Architecture Decision (Pending)

**Option A: On-Device (Preferred)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Flutter App                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Style Input â”‚â”€â”€â”€â–¶â”‚ On-Device Modelâ”‚ â”‚
â”‚  â”‚ (15 chars)  â”‚    â”‚ (Core ML/TFLite)â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                             â”‚           â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                    â”‚ Live Preview UI â”‚ â”‚
â”‚                    â”‚ (step-by-step)  â”‚ â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                             â”‚           â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                    â”‚  Vectorization  â”‚ â”‚
â”‚                    â”‚  + Animation    â”‚ â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
- Pros: Works offline, no server costs, lower latency
- Cons: ~400MB download, flagship phones only

**Option B: Server-Side with Streaming**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flutter App  â”‚â”€â”€â”€â”€â–¶â”‚  API Gateway â”‚â”€â”€â”€â”€â–¶â”‚  GPU Server  â”‚
â”‚              â”‚â—€â”€â”€â”€â”€â”‚  (WebSocket) â”‚â—€â”€â”€â”€â”€â”‚  (SageMaker) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Live preview          Stream            Inference
     animation           each step           (~2-3 sec)
```
- Pros: Works on any phone, smaller app
- Cons: Requires internet, server costs (~$0.02/word)

**Decision:** Test on-device first after training completes. Fall back to server if performance insufficient.

### ðŸ“Š Handwriting AI Models Research (Jan 21, 2026)

**Criteria:**
- Open-source/free
- Few-shot capable (1-20 samples without retraining)
- Vector/stroke output OR high-quality raster
- English handwriting support

**17 Models Identified:**

| Model | Year | Output | Few-shot | Status | Notes |
|-------|------|--------|----------|--------|-------|
| **SDT** | 2023 | Strokes | âŒ (needs 15) | âŒ FAILED | Illegible English output |
| **One-DM** | 2024 | Raster | âœ… (1 sample) | âŒ FAILED | Wrong characters ("Tea"â†’"alea") |
| **DiffusionPen** | 2024 | Raster | âœ… (few-shot) | Not tested | SOTA quality claimed |
| **InkSight** | 2024 | Strokes | âŒ | Not tested | Vectorization tool, not generator |
| **DiffBrush** | 2023 | Raster | âœ… | Not tested | Stroke-level control |
| **Diff-Handwriting** | 2023 | Raster | âœ… | Not tested | Diffusion-based |
| **FW-GAN** | 2022 | Raster | âœ… | Not tested | GAN-based |
| **GANwriting** | 2020 | Raster | âœ… | Not tested | GAN-based |
| **HWT** | 2022 | Raster | âŒ (paired) | Not tested | Paired style transfer |
| **SmartPatch** | 2023 | Raster | âŒ | Not tested | Patch-based |
| **handwriting-synthesis** | 2013 | Strokes | âŒ (retrain) | Not tested | Graves RNN, direct vector |
| **DeepWriting** | 2016 | Strokes | âŒ (retrain) | Not tested | Extended Graves model |
| **Handwriting Transformer** | 2021 | Strokes | âŒ (retrain) | Not tested | Transformer-based |
| **Text-to-Handwriting** | 2020 | Raster | âŒ | Not tested | Pix2pix GAN |
| **VATr** | 2021 | Raster | âŒ | Not tested | Vision-audio transformer |
| **Write Like Me** | 2021 | Raster | âŒ | Not tested | Commercial API reference |
| **CalliGAN** | 2020 | Raster | âŒ | Not tested | Calligraphy-focused |

**Key Insights:**
- **Modern models** (2023-2024): Diffusion-based, raster output, best quality, few-shot capable
- **Classic models** (2013-2016): RNN-based, stroke output, require full retraining
- **No perfect match**: No open-source model has vector + few-shot + proven accuracy
- **Hybrid approach needed**: Best modern model + vectorization tool (InkSight)

**Concern:** One-DM (the most recent diffusion model) already failed accuracy tests. Other diffusion models may have similar issues.

## Project Structure

```
glossy/
â”œâ”€â”€ app/                    # Flutter mobile app (not started)
â”œâ”€â”€ backend/                # AWS Lambda functions (not started)
â”œâ”€â”€ docs/                   # Additional documentation
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ SDT/                # SDT model (CVPR 2023) - âŒ English output illegible
â”‚   â”‚   â”œâ”€â”€ model_zoo/      # Pre-trained models (261 MB + 69 MB)
â”‚   â”‚   â”œâ”€â”€ data/           # CASIA English dataset (204 writers)
â”‚   â”‚   â”œâ”€â”€ Generated/      # Test output (1,984 samples - illegible)
â”‚   â”‚   â”œâ”€â”€ venv/           # Python 3.11 environment
â”‚   â”‚   â””â”€â”€ *.py            # Test and visualization scripts
â”‚   â””â”€â”€ One-DM/             # One-DM model (ECCV 2024) - âœ… SUCCESS (legible output!)
â”‚       â”œâ”€â”€ model_zoo/      # Pre-trained models (1.6 GB - downloaded)
â”‚       â”œâ”€â”€ data/           # IAM English dataset + text corpus
â”‚       â”œâ”€â”€ Generated/      # Test output (5 samples - all legible!)
â”‚       â”œâ”€â”€ venv/           # Python 3.11 environment
â”‚       â”œâ”€â”€ test_macos.py   # macOS-compatible test script
â”‚       â””â”€â”€ *.py            # Model scripts
â””â”€â”€ scripts/                # Utility scripts (not started)
```

## Technical Notes

**SDT Model Details:**
- Input: 15 style samples (64x64 grayscale) + target character (64x64)
- Output: Up to 120 stroke points with 5D coordinates
- Coordinate format: Incremental (dx, dy) + pen state (up/down/end)
- For AxiDraw: Convert to absolute coordinates via cumulative sum

**Environment:**
- Python: 3.11.14
- PyTorch: 2.2.2 (with MPS support for Apple Silicon)
- Device: macOS with Apple Silicon GPU
- Dependencies: numpy 1.26.4, opencv-python 4.5.5, matplotlib 3.7.5

**Repository:**
- GitHub: https://github.com/justinmvail/glossy
- Created: Jan 21, 2026
- Current status: Model evaluation phase

---

### âœ… WordSDT Training (Jan 25, 2026 - IN PROGRESS)

**Pivot:** Instead of using the original SDT checkpoint, we're training a custom word-level model from scratch on synthetic font data.

**Architecture - WordSDT (58.6M params):**
- Variable-width content encoder (ResNet + Transformer)
- Style encoder with writer/glyph heads  
- GMM-based stroke decoder (20 mixtures)
- Outputs: (dx, dy, pen_state) for pen plotter

**Data Pipeline:**
1. Downloaded 52 Google Fonts (handwriting category)
2. Converted to single-line via skeletonization
3. Combined with 41 existing EMS/Hershey fonts = 93 base fonts
4. Applied style transforms (slant, jitter, tremor) to create 1000 synthetic writers
5. Generated 146,988 training samples

**Current Training:**
- Dataset: 10,742 samples (small initial run)
- Epoch: ~70/100
- Best val loss: -0.7518 (epoch 62)
- Hardware: GTX 1660 Super (6GB)

**Key Files:**
- Model: `/home/server/glossy/sdt_word/models/word_model.py`
- Training: `/home/server/glossy/sdt_word/train.py`
- Pipeline docs: `/home/server/glossy/sdt_word/PIPELINE.md`

**Next Steps:**
1. Complete current training run
2. Test generation quality
3. Retrain on larger 147K dataset
4. Scale up cloud training if promising

**Why This Might Work:**
- Single-line fonts already have stroke data (no vectorization needed)
- Style transforms create realistic variation from limited fonts
- Word-level output matches our use case
- Can control data quality (vs relying on IAM-OnDB access)

---

### ðŸ–Šï¸ InkSight Font Vectorizer (Jan 26, 2026)

**Purpose:** Convert outline fonts to single-line vector strokes for use in handwriting generation and pen plotting.

**How it Works:**
1. Render font word as image (PIL)
2. Run Google's InkSight model (offline-to-online handwriting converter)
3. Post-process strokes: filter artifacts, Gaussian smoothing, extend endpoints to connect gaps
4. Optionally validate with TrOCR OCR (if readable = good vectorization)

**InkSight Model:**
- Google Research model for converting handwriting images to digital ink
- Token format: 450 = start stroke, 0-224 = x coord, 225-449 = y coord
- Location: `/home/server/inksight/model/` (TensorFlow SavedModel)

**Post-Processing Pipeline:**
- `smart_filter()`: Remove single-point edge artifacts
- `smooth_gaussian()`: scipy.ndimage gaussian_filter1d (sigma=1.5)
- `extend_to_connect()`: Ray-cast from endpoints to connect nearby strokes (max 8px)

**OCR Validation (Optional):**
- Uses Microsoft TrOCR (`trocr-base-handwritten`)
- Renders strokes as image, runs OCR, compares to expected word
- Threshold: 80% similarity = PASS
- Note: Slow on CPU (~5 min/sample due to TF/PyTorch GPU conflict)

**Usage:**
```bash
conda activate inksight
cd /home/server/glossy

# Single word with visualization
python font_scraper/inksight_vectorizer.py --font path/to/font.ttf --word "Hello" --show

# With OCR validation
python font_scraper/inksight_vectorizer.py --font path/to/font.ttf --word "Hello" --validate

# Process full charset
python font_scraper/inksight_vectorizer.py --font path/to/font.ttf --charset --output ./output/
```

**Key Files:**
- `font_scraper/inksight_vectorizer.py` - Main vectorizer + OCR validator
- Classes: `InkSightVectorizer`, `OCRValidator`, `Stroke`, `InkResult`

**Test Results (10 fonts Ã— 10 words):**
- Clean fonts (Great Wishes, Bristol, Daydreamer): Excellent results
- Script fonts: Good with some gaps
- Decorative/signature fonts: Variable quality
- OCR validation filters out unreadable results

**Dependencies:**
```
tensorflow>=2.15
tensorflow-text
transformers  # for TrOCR validation
torch         # for TrOCR validation
```

---

### ðŸ–¥ï¸ GPU Setup for TensorFlow + PyTorch (Jan 26, 2026)

**Problem:** TensorFlow and PyTorch have conflicting CUDA library requirements:
- TensorFlow 2.18 needs cuDNN 9.3+
- PyTorch 2.6 (CUDA 12.4) ships with cuDNN 9.1

**Solution:** Install PyTorch with CUDA 12.4, then upgrade cuDNN to satisfy TensorFlow.

**âš ï¸ Important:** TensorFlow and PyTorch cannot be loaded in the same Python process (segfault). Use subprocess isolation for OCR validation.

#### Prerequisites

- NVIDIA GPU with compute capability 7.5+ (tested on GTX 1660 Super)
- NVIDIA driver 535+ installed
- Python 3.12

#### Step-by-Step Setup

```bash
# 1. Install TensorFlow 2.18 (for InkSight)
pip install --break-system-packages tensorflow==2.18.0 tensorflow-text

# 2. Install PyTorch with CUDA 12.4 (for TrOCR)
pip install --break-system-packages torch torchvision --index-url https://download.pytorch.org/whl/cu124

# 3. Upgrade cuDNN to satisfy TensorFlow (PyTorch installs 9.1, TF needs 9.3+)
pip install --break-system-packages 'nvidia-cudnn-cu12>=9.3.0'

# 4. Fix numpy version (PyTorch installs 2.x, TF needs <2.1)
pip install --break-system-packages 'numpy<2.1.0,>=1.26.0'

# 5. Install transformers for TrOCR
pip install --break-system-packages transformers
```

#### Verification

```bash
# Test TensorFlow GPU (run separately)
python3 -c "
import tensorflow as tf
print('TensorFlow:', tf.__version__)
print('GPU:', tf.config.list_physical_devices('GPU'))
"
# Expected: GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]

# Test PyTorch GPU (run separately)
python3 -c "
import torch
print('PyTorch:', torch.__version__)
print('CUDA:', torch.cuda.is_available())
print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A')
"
# Expected: CUDA: True, GPU: NVIDIA GeForce GTX 1660 SUPER
```

#### Final Package Versions (Working Configuration)

```
tensorflow==2.18.0
torch==2.6.0+cu124
torchvision==0.21.0+cu124
transformers==5.0.0
numpy==2.0.2
nvidia-cublas-cu12==12.4.5.8
nvidia-cuda-cupti-cu12==12.4.127
nvidia-cuda-nvrtc-cu12==12.4.127
nvidia-cuda-runtime-cu12==12.4.127
nvidia-cudnn-cu12==9.18.1.3  # Upgraded from PyTorch's 9.1.0
nvidia-cufft-cu12==11.2.1.3
nvidia-curand-cu12==10.3.5.147
nvidia-cusolver-cu12==11.6.1.9
nvidia-cusparse-cu12==12.3.1.170
nvidia-nvjitlink-cu12==12.4.127
```

#### Subprocess Isolation for OCR

TensorFlow and PyTorch crash when loaded together. The `OCRValidator` class in `inksight_vectorizer.py` uses subprocess isolation:

1. Main process: Loads TensorFlow for InkSight vectorization
2. Subprocess: Loads PyTorch for TrOCR OCR validation
3. Communication: Base64-encoded images over stdin/stdout

This allows both frameworks to use GPU without conflicts.

#### Troubleshooting

**"Cannot dlopen some GPU libraries"**
- cuDNN version mismatch. Run: `pip install --break-system-packages 'nvidia-cudnn-cu12>=9.3.0'`

**"Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0"**
- Same fix as above.

**Segmentation fault when importing both TF and PyTorch**
- Expected behavior. Use separate processes (subprocess isolation already implemented).

**PyTorch "Illegal instruction" crash**
- CPU doesn't support required instructions. Use GPU PyTorch (cu124) instead of CPU-only version.

#### Hardware Tested

| Component | Spec |
|-----------|------|
| GPU | NVIDIA GeForce GTX 1660 SUPER (6GB) |
| Driver | 535.x |
| CUDA | 12.4 (via pip nvidia packages) |
| cuDNN | 9.18.1.3 |
| OS | Ubuntu 24.04 (Linux 6.8.0) |

